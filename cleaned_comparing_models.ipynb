{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "9NW8z_38xaGl"
   },
   "outputs": [],
   "source": [
    "!pip install transformers torchvision Pillow gtts --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2q_qVkGoyKWA"
   },
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    VisionEncoderDecoderModel, ViTImageProcessor, AutoTokenizer,\n",
    "    BlipProcessor, BlipForConditionalGeneration\n",
    ")\n",
    "import torch\n",
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "16b9a04c49204416a3feff89cd61a01a",
      "d93accd72ee243d691bc2b3e880d3ec8",
      "f5dc9503c45f475a9fa2a3d417a96511",
      "1b3e4259ef2d4829bab7d3a2501c40ae",
      "8bfea6ed1eac4fd99db04137ddb2cf19",
      "078217639d3344dfb30a99f6e702ce88",
      "4f018a40723244e196670523a06b14bd",
      "00e31ce2ce82475489e232970de2bb16",
      "b7fc1acc421b4b548d45775b3e2c5694",
      "cd8768a733fa40df9c1c69404d0d2dff",
      "a7911a2486f84ee299aa2b91fc1859a0",
      "369ed5e5ed04403dad160182a3110199",
      "411a4d71e79546b2bf6db292a72006b4",
      "8c487f52dc994180a8ad3d6840f0baa4",
      "0775ecd448334a63805f4ce6afa31e6e",
      "04069bb1ecad48f08f8279f5ddc718f2",
      "41aecb533c974b41a193ee6b5f9d461f",
      "5ef816addb84466eb9dcdd20a47cae01",
      "dc5156c21d014ebd888e7ad98c26f450",
      "2fd93988872d452cbd28e0a292f0eb14",
      "fe2657b7571c488885789fa2ee0637f0",
      "90d747188d5c4a7480cde1d989415979",
      "3f8b4a7a36cc4e30af878a025e834b01",
      "a00d8d44a12947e191261231dea7c58f",
      "61cf2e61e21b41faa4a5a2bdd1205f7f",
      "9bb78104d8c044ba89ec7e1ca61db698",
      "cb925a9d31e9427b902cd356b9f1f95d",
      "e30585999d8642c4bab65e59a5c09686",
      "84315e6d33ba447b82f44b50dd8438af",
      "74d942aee6e042a1b0d241d9bbeeb563",
      "87ab60f8a4e04178916a8e02b5dbc3da",
      "2a828851553f4bcc8cb657cbf49d51b1",
      "769c5ab5b34f42a8893dfbdaaafcfa5a",
      "4be91a2af6c345878f288f0fcbee5a65",
      "88a229144e2f4113b41cb6c2ac441e86",
      "279bdc4534944348aec4644e66c68f6b",
      "833c6d35aa8147e2b6ed4d7336ad4cbb",
      "ae1f4688055042bda81ce9b70e420f3a",
      "0871259c3aa049d4a4a5b9bc5aa02447",
      "3a165729774e4df69c869bddde1f3ac6",
      "e81215ddf26843e891e63944a2a23459",
      "37e47ed3755747e3a597cb4ad9b0a852",
      "476aadef109c45d0b3b780bc214d977d",
      "308bcc3f121d449ab3c29b974194e860",
      "6bde55b16c5a418e8f6b3fbc528de493",
      "05511f39b9ba44e3a137d8183e6d87b9",
      "ffb12fd12e4b4be4aca157c7d421d45d",
      "b17b5207227d45228c2236884921bd4d",
      "4873a05cf00844fbbecb0ce07fdc1a5a",
      "1ba550dc19fb4216b0a7199d01e2f25c",
      "a0649866630f46b1b7f04c3920f0d656",
      "0993a3a653f24b14bf77f863b4aced57",
      "42d12d317a684ba697954a18da3d2154",
      "6080237fc17743569cb83bf816f6c0a9",
      "f4e5744208fd46ea989524e0621ea4b1",
      "0b9c29fc81c54f379e47c67e5332d392",
      "5b6745f219bf4675b5200eae3c47864c",
      "81d088462007424db4a9b5a2bdc50539",
      "ab922ec9bfc54ca7a25f7a5867575e43",
      "627823d527dd4d4b81f2881f2bf11beb",
      "1bf59985e69d46eeb96d30670904f52f",
      "8b0cf91dc90849e4b0c1dbf5e36bd7ed",
      "6c26fc6a5ebd4a85a8b1aa86a38d109f",
      "d6abf4b0872346218019283ce54de16b",
      "478f6ce1b00b400283905ae6d26fea52",
      "7ff058ce5db44ad7a186b7dfddb3eca4",
      "79fd76c4b1634465a4f6d702ac37d50b",
      "f43eb34245b34ae2bb518e3c46a241fb",
      "48079ca040cf41eab479715197acfc34",
      "883359252e4b4eb982e775592405e589",
      "444490bbceb6426fbac7a13a66ca2b6f",
      "a7fd7400fde3437f9abfc7aa49428e95",
      "32c15bc403d54843ae67e0cd7e0809ae",
      "a76e734ffe6e4ec68c818091adf79653",
      "2cc608cbcaa341a297add9b6a24ad128",
      "7a542ed9473b4602a10c6d4b67f53788",
      "4be3bb5b27ce41f3a05eb701b16262e0",
      "88ef579d09a64bd29e08529f81368eda",
      "8d9964b8c0f04746bd65552e0aeca8eb",
      "d74563bd059047828b01644410deca06",
      "388bf3eb86f34d79902da5ee8e1c884b",
      "b7e33a57fa654ac5b05f0832a7e5ffa9",
      "5dfbd24259d34c5fbca57d3e00b46888",
      "e860f984ae37490c94c2d08b48a3a211",
      "90e19f15342c471ab7b393ec3274bf63",
      "c653e5adaf5a40f79ae311b3d1a28f76",
      "13fde90e28bd44fdb1712a74d0436e4c",
      "a033384b06ba4ef980c56432b41fd8cc",
      "840ac8fd3c6344e98a666d29a3347f3b",
      "6099e070d0854bb88597bd3aed367818",
      "9379deadc6394b858f2b953341a76565",
      "892f74d0dd7441949ca7d45ec79426e5",
      "bd974ea09cfb41b8a59be31a64b6dc3f",
      "96204def06fe43878f88155f975da491",
      "1845417c15db411296d17dd2663d236c",
      "c4fbea606bc84b80b4ddacd97336e490",
      "a6930c4672c34cae8c9e01cbcef82bdb",
      "f4de1dadf291457c8e0e7b0d1935120d",
      "2581e2fe4cfd4ea4a0f69b0857d82f39",
      "3b4ca7a61df542ce90760e93496b1dcb",
      "8b03187e3a064222ad4f603d6f2c68c3",
      "87029c4cec2d401dbe3eeccde5605811",
      "fc02541a52014efcb2e1c44ef86ec905",
      "3eb9ba90b43e43f682a6a3f591e35582",
      "bf479799ec3048969434be5d27d9de42",
      "ff577ed9fce2426ca96b3451e9fb5021",
      "b41e31bb62784c528ccacd735e5bbfbe",
      "c34340d097404da9b214dc44e131821e",
      "872d7467311b45eab3f3aeb4db60db53",
      "02e4a9ccc0de4aa3b3b3fedb12706e49",
      "75aa25c407244082b166ac4bd6c4b250",
      "0c32b033854745629d363e25191b7b1b",
      "656a0f118fab4c0d8be7584c368cfbfc",
      "6821342b35434cec8f882318c109a49f",
      "d682410884e3438f9a7295f68da22f5d",
      "bcfc1027c1f74f3e9b36cdcbe5aa73aa",
      "cc0a9b29f244411eb61bec3908d28a0f",
      "ca0d33269dd14034830333832416c7a8",
      "453fdc87c32c40c69cf5c0b51823bc32",
      "9ca19a1d15694cce9835dcd18a8ed822",
      "93bb7b0b2f4b4a07a1f3bdfd42f1a502",
      "f9067164909943beb96d84d460b09647",
      "6e6a911a1aa5435e84c39a10358a55f6",
      "58ad067121ce426696d03fa7807ae9a2",
      "4db1ed421bd44841931ae93c8682468f",
      "f488a97d9c4c4475bdbaf93df8c24f5d",
      "0931d2ac826d428c937140c1317ff557",
      "ba7987bbc54946e4b68b7332b07f0177",
      "a529b21665934975847b344beb1262df",
      "74756fa11ab0422d9bb178c4affd819c",
      "60a1e0aa131542ff8a7c3f0b693f55c4",
      "2377f8a2fc73481b87cce2c5ae5fd336",
      "3be70afa93ba460c837b2d2fac348b55",
      "a82711235e524dbfb10fc331bcc2f51d",
      "abadbada804348d684975cb9884be85f",
      "6fc4a1b3441f40a9a6b650a10f104ce1",
      "245a572f62e64e1d9db8eee238c27db1",
      "fdd89c47491847aba9dd68cf56251b1d",
      "f22444e1ccb141fdb787ae5bd394c0b2",
      "023fc483f664481c9c78e1a9d27612b4",
      "20ed5e0705534c87b8d43b51bac51bfc",
      "0ea143927b214acfae368e7c852f6bb8",
      "133f91d9f7b54c15899da6ddfffd27ca",
      "5a4ed1eb42d44609947d8551a57087f4",
      "1d1f2ba78e8b494d9488e05178a8dd01",
      "169088ba4fc14ecdba23d455c81559fb",
      "2194769080e54299ae93aff0a60315f2",
      "5c176522b6fa403cba3c3d5fc5d1fd48",
      "3c06937e74bc4688beae2610d8768acb",
      "22653b1bbe804bdf83cca5d05973ea7a",
      "429f3dee48aa4c849f33f6d83edfc6f4",
      "05a7f28e71c448c19fdcb961ba508b31",
      "e554185078ec495895c8a29bce85affc",
      "b83a124de9094208b25cd74efcec5990",
      "4ffb5336e9ba453d95e0e7ffcb530257",
      "cff5dc21dbff4da6a4acd6c546aea02e",
      "c499a79f8415474880223f3a566ff027",
      "07df7afcb849498d92cbf598f15639b2",
      "6f8169b34d9d49cba90c9d992d51c38d",
      "245e640052774ac6a8b4aae62611c9a1",
      "67dfc1a4628b470c8f0b8f53af3341bb",
      "804394bb94474ebbacabf0c9a0f346c0",
      "c1328950a8b947a1af24c51ee86a79d9",
      "b5ab67896078461da2272590b8f45e99",
      "1092fbd776934d8ea2b3093c0af4bfdd",
      "96fbb18e6ad54090910681fde1a1b357",
      "1eb4238a501444e4b621f9ef695b8d93",
      "0a9a175430804757bc8c3439e44bd055",
      "12ad78e57897459eb2d551a731e16ce9",
      "8f14cac347374c7390d5184670c8bcf6",
      "2a4cd23a2fe64be28cc0f8acc97f8e6b",
      "d7753798aa1d402b8d91dccfa1afdbe3",
      "2b2687b311d04ca29b9c859735ff3205",
      "0076c2c2ba864e91b73e63590a4cae21",
      "162de29b7dbf40a5aa81544b6bff4b65",
      "6b0fb0077f604ecdb8b24ca26cf43218",
      "30c1b421f791407c99aff57f59aa47c6",
      "d73056b91b8e43659630af4f57b0f27a",
      "895d3c3ffb984643a28cda4c53f83efe",
      "506a557e9719465983d25efd2c257f4e",
      "db8d49cb3d924988a190f0a878191a54",
      "3468aaf6b78a4c919c9e8dfaaa4eca73",
      "8595d631410c43de87d022d6478a4577",
      "443a0e65b3234302b57eeb04f920cea5",
      "75f43a3040b1498e90648508eba0276c",
      "ee1407424ac34fb89eaef2c3e3a8376f",
      "4c843e6bc9fe4a33a614b572adb3d7df"
     ]
    },
    "id": "2Ty3sqE6yKAZ",
    "outputId": "826e41f3-230a-42da-fdc9-0548a336cd5a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16b9a04c49204416a3feff89cd61a01a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/4.61k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "369ed5e5ed04403dad160182a3110199",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/982M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f8b4a7a36cc4e30af878a025e834b01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/982M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Config of the encoder: <class 'transformers.models.vit.modeling_vit.ViTModel'> is overwritten by shared encoder config: ViTConfig {\n",
      "  \"architectures\": [\n",
      "    \"ViTModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"encoder_stride\": 16,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 768,\n",
      "  \"image_size\": 224,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"model_type\": \"vit\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_channels\": 3,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"patch_size\": 16,\n",
      "  \"pooler_act\": \"tanh\",\n",
      "  \"pooler_output_size\": 768,\n",
      "  \"qkv_bias\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.51.3\"\n",
      "}\n",
      "\n",
      "Config of the decoder: <class 'transformers.models.gpt2.modeling_gpt2.GPT2LMHeadModel'> is overwritten by shared decoder config: GPT2Config {\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"add_cross_attention\": true,\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"decoder_start_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"is_decoder\": true,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"pad_token_id\": 50256,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4be91a2af6c345878f288f0fcbee5a65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/228 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bde55b16c5a418e8f6b3fbc528de493",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/241 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b9c29fc81c54f379e47c67e5332d392",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/798k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79fd76c4b1634465a4f6d702ac37d50b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88ef579d09a64bd29e08529f81368eda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "840ac8fd3c6344e98a666d29a3347f3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/120 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b4ca7a61df542ce90760e93496b1dcb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/287 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75aa25c407244082b166ac4bd6c4b250",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/506 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9067164909943beb96d84d460b09647",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3be70afa93ba460c837b2d2fac348b55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/711k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a4ed1eb42d44609947d8551a57087f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ffb5336e9ba453d95e0e7ffcb530257",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/4.56k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96fbb18e6ad54090910681fde1a1b357",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/990M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30c1b421f791407c99aff57f59aa47c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/990M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ViT-GPT2\n",
    "vit_model = VisionEncoderDecoderModel.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\").to(device)\n",
    "vit_processor = ViTImageProcessor.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n",
    "vit_tokenizer = AutoTokenizer.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n",
    "\n",
    "# BLIP\n",
    "blip_processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "blip_model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PTjvqolZziXg"
   },
   "outputs": [],
   "source": [
    "img_url = \"https://images.pexels.com/photos/1108099/pexels-photo-1108099.jpeg\"\n",
    "image = Image.open(requests.get(img_url, stream=True).raw).convert(\"RGB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d2BHk7BLzqKP",
    "outputId": "c3c6ffdb-24e7-4011-9de1-d8139c32951b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n",
      "You may ignore this warning if your `pad_token_id` (50256) is identical to the `bos_token_id` (50256), `eos_token_id` (50256), or the `sep_token_id` (None), and your input is not padded.\n"
     ]
    }
   ],
   "source": [
    "# ViT-GPT2 Caption\n",
    "vit_inputs = vit_processor(images=image, return_tensors=\"pt\").pixel_values.to(device)\n",
    "gen_kwargs = {\"max_length\": 16, \"num_beams\": 4}\n",
    "vit_output = vit_model.generate(vit_inputs, **gen_kwargs)\n",
    "caption_vit = vit_tokenizer.decode(vit_output[0], skip_special_tokens=True).strip()\n",
    "\n",
    "# BLIP Caption\n",
    "blip_inputs = blip_processor(images=image, return_tensors=\"pt\").to(device)\n",
    "blip_output = blip_model.generate(**blip_inputs)\n",
    "caption_blip = blip_processor.decode(blip_output[0], skip_special_tokens=True).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1o8ixKb51IdY",
    "outputId": "2dc7ca07-4c70-4435-8c13-9ae549c6d93d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Caption Comparison\n",
      "\n",
      "ViT-GPT2: two dogs are sitting in a field with flowers\n",
      "BLIP:     two pup sitting in a field of flowers\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nCaption Comparison\\n\")\n",
    "print(f\"ViT-GPT2: {caption_vit}\")\n",
    "print(f\"BLIP:     {caption_blip}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  },
  "widgets": {
   "state": {},
   "version_major": 2,
   "version_minor": 0
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
